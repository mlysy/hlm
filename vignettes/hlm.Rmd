---
title: "Fitting the Heteroscedastic Model"
author: "Martin Lysy"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Fitting the Heteroscedastic Model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\tx}[1]{\textrm{#1}}
\newcommand{\ind}{\stackrel{\textrm{ind}}{\sim}}
\newcommand{\iid}{\stackrel{\textrm{iid}}{\sim}}
\newcommand{\N}{\mathcal N}
\newcommand{\rr}{{\bm r}}
\newcommand{\zz}{{\bm z}}
\newcommand{\ZZ}{{\bm Z}}
\newcommand{\WW}{{\bm W}}
\newcommand{\yy}{{\bm y}}
\newcommand{\ww}{{\bm w}}
\newcommand{\gg}{{\bm g}}
\newcommand{\HH}{{\bm H}}
\newcommand{\gga}{{\bm \gamma}}
\newcommand{\FI}{\bm{\mathcal I}}
\newcommand{\veps}{\varepsilon}
\newcommand{\diag}{\operatorname{diag}}


## Setup

The simplified heteroscedastic model is of the form
$$
y_i \ind \N\big(0, \exp(\zz_i'\gga)\big), \qquad i = 1,\ldots,n.
$$
The loglikelihood is given by
$$
\ell(\gga ) = -\frac 1 2 \sum_{i=1}^n \left[\frac{ y_i^2}{\exp(\zz_i'\gga)} + \zz_i'\gga\right].
$$
Its gradient and Hessian are given by
$$
\begin{aligned}
\gg(\gga) & = \frac{\partial \ell(\gga )}{\partial \gga}  = \frac 1 2 \sum_{i=1}^n \left[\frac{y_i^2}{\exp(\zz_i'\gga)} - 1\right]\zz_i \\
\HH(\gga) & = \frac{\partial^2 \ell(\gga )}{\partial \gga\partial \gga'}  = - \frac 1 2 \sum_{i=1}^n \frac{y_i^2 \zz_i\zz_i'}{\exp(\zz_i'\gga)}.
\end{aligned}
$$

## Parameter Estimation

### Fisher Scoring

The expected Fisher information is
$$
\FI(\gga) = -E[\HH(\gga)] = \frac 1 2 \sum_{i=1}^n \frac{E[y_i^2] \zz_i\zz_i'}{\exp(\zz_i'\gga)} = \tfrac 1 2 \ZZ'\ZZ.
$$
Therefore, the [Fisher scoring](https://en.wikipedia.org/wiki/Scoring_algorithm) algorithm is
$$
\gga_{m+1} = \gga_m + \FI(\gga_m)^{-1} \gg(\gga_m).
$$
An initial value can be found by noting that
$$
r_i = \log(y_i^2) = \zz_i'\gga + \veps_i, \qquad \exp(\veps_i) \iid \chi^2_{(1)}.
$$
Since
$$
\rho = E[\veps_i] =  \psi(\tfrac 1 2) + \log 2,
$$
where $\psi(x)$ is the [digamma function](https://en.wikipedia.org/wiki/Digamma_function), an initial value for $\gga$ is given by the linear regression estimator
$$
\gga_0 = (\ZZ'\ZZ)^{-1}\ZZ'(\rr - \rho).
$$
Following the `stats::glm.fit()` function in R, the stopping criterion for the algorithm is either when more than $M_{\tx{max}}$ steps have been taken, or when
$$
\frac{|\ell(\gga_{m} ) - \ell(\gga_{m-1} )|}{0.1 + |\ell(\gga_m )|} < \epsilon.
$$

### Iteratively Reweighted Least Squares

A different estimation algorithm is obtained by noting that the score function may be written as
$$
\gg(\gga) = \frac 1 2 \sum_{i=1}^n w_i \cdot (r_i - \zz_i'\gga), \qquad w_i = \frac{\exp(r_i - \zz_i'\gga) - 1}{r_i - \zz_i'\gga}.
$$
Thus we have $\gg(\gga) = \tfrac 1 2 \WW(\rr - \ZZ\gga)$ where $\WW = \diag(w_1, \ldots, w_n)$.  Setting the score function to zero produces the [iteratively reweighted least squares](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) algorithm
$$
\gga_{m+1} = (\ZZ'\WW_m \ZZ)^{-1}\ZZ'\WW_m \rr, \qquad \WW_m = \WW(\gga_m).
$$
